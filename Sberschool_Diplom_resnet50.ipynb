{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sberschool_Diplom_resnet50.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNBTMcVhcMZ0YWWig4BMnQg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapitalistka/MLScannerApp/blob/master/Sberschool_Diplom_resnet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initializations"
      ],
      "metadata": {
        "id": "GodmwUE8c8EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms, models\n",
        "from torchvision.utils import make_grid\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import cv2\n",
        "from PIL import ImageFile\n",
        "import torchvision.transforms as transforms\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "%matplotlib inline\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /\n",
        "%cd /content/gdrive/MyDrive/ML/breeds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgKoR3ekc8f0",
        "outputId": "6c70edc4-a376-40f3-843e-46059d636583"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/\n",
            "/content/gdrive/MyDrive/ML/breeds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA"
      ],
      "metadata": {
        "id": "YwuiXH1IdMaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the dataset \n",
        "PATH = ''\n",
        "labels = pd.read_csv('labels.csv')\n",
        "labelnames = pd.read_csv('sample_submission.csv').keys()[1:]\n",
        "print(\"Train folder has \", len(os.listdir('train')),'images which matches with label\\'s', len(labels),'images')\n",
        "\n",
        "codes = range(len(labelnames))\n",
        "breed_to_code = dict(zip(labelnames, codes))\n",
        "code_to_breed = dict(zip(codes, labelnames))\n",
        "labels['target'] =  [breed_to_code[x] for x in labels.breed]\n",
        "labels['rank'] = labels.groupby('breed').rank()['id']\n",
        "labels_pivot = labels.pivot('id', 'breed', 'target').reset_index().fillna(0)\n",
        "\n",
        "#Split to train and valid\n",
        "train = labels_pivot.sample(frac=0.85)\n",
        "valid = labels_pivot[~labels_pivot['id'].isin(train['id'])]\n",
        "print(\"Train shape is \", train.shape, \". Validation  shape is \", valid.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FX8bv4endPLv",
        "outputId": "b5af8a69-3aaf-4e20-ad27-2552a6b78ae6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train folder has  10222 images which matches with label's 10222 images\n",
            "Train shape is  (8689, 121) . Validation  shape is  (1533, 121)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data preparation"
      ],
      "metadata": {
        "id": "KbM1v53GemEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Image transformations\n",
        "img_transform = {\n",
        "    'valid':transforms.Compose([\n",
        "        transforms.Resize(size = 256),\n",
        "        transforms.CenterCrop(size = 224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'train':transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size = 256),\n",
        "        transforms.RandomRotation(degrees = 30),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  \n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  \n",
        "    ]),\n",
        "    'test':transforms.Compose([\n",
        "        transforms.Resize(size = 256),\n",
        "        transforms.CenterCrop(size = 224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        " # Train and valid dataset\n",
        "class DogBreedDataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, img_dir, label, transform):\n",
        "        'Initialization'\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label = label\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.label is not None:\n",
        "            img_name = '{}.jpg'.format(self.label.iloc[index, 0])\n",
        "            fullname = self.img_dir + img_name\n",
        "            image = Image.open(fullname)\n",
        "            label = self.label.iloc[index, 1:].astype('float').to_numpy()\n",
        "            label = np.argmax(label)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return [image, label]\n",
        "\n",
        "batch_size = 16\n",
        "num_workers = 8\n",
        "\n",
        "train_img = DogBreedDataset('train/', train, transform = img_transform['train'])\n",
        "valid_img = DogBreedDataset('train/', valid, transform = img_transform['valid'])\n",
        "\n",
        "dataloaders={\n",
        "    'train' : torch.utils.data.DataLoader(train_img, batch_size, num_workers = num_workers, shuffle = True),\n",
        "    'valid' : torch.utils.data.DataLoader(valid_img, batch_size, num_workers = num_workers, shuffle = False)\n",
        "}"
      ],
      "metadata": {
        "id": "oSl2fSdierX9"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training\n"
      ],
      "metadata": {
        "id": "YcRXWmr3fVSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchsummary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCTt6LRofaZ2",
        "outputId": "ddb529e0-1105-4355-eeb8-5962cf82730f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (1.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda:0')\n",
        "else:\n",
        "  device = torch.device('cpu')\n",
        "\n",
        "def train(n_epochs, loaders, model, optimizer, criterion, scheduler, name_for_save):\n",
        "    valid_loss_min = np.Inf \n",
        "\n",
        "    VALID_LOSES = np.zeros((n_epochs,))\n",
        "    TRAIN_LOSES = np.zeros((n_epochs,))\n",
        "    ACCURACIES = np.zeros((n_epochs,))\n",
        "    ACCURACIES_TRAIN = np.zeros((n_epochs,))\n",
        "    \n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "        \n",
        "        ###################\n",
        "        # train the model #\n",
        "        ###################\n",
        "        model.train()\n",
        "        batch_counter = 0\n",
        "        correct = 0\n",
        "        \n",
        "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
        "            batch_counter += 1\n",
        "\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            \n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
        "            y_pred = output.max(1)[1].detach().cpu().numpy()\n",
        "            correct_batch= np.mean(target.cpu().numpy() == y_pred)\n",
        "            correct  += np.mean(target.cpu().numpy() == y_pred)\n",
        "           \n",
        "            if batch_idx % 100 == 0:\n",
        "                print('Epoch: %d \\tBatch: %d \\tTraining Loss: %.6f' %(epoch, batch_idx + 1, train_loss))\n",
        "\n",
        "\n",
        "        ACCURACIES_TRAIN[epoch - 1] = correct / batch_counter\n",
        "        print(f\"Train accuracy: { ACCURACIES_TRAIN[epoch - 1] }, batches count:  {batch_counter}\")\n",
        " \n",
        "        ######################    \n",
        "        # validate the model #\n",
        "        ######################\n",
        "\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        batch_counter = 0  \n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
        "            batch_counter += 1\n",
        "            \n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
        "            \n",
        "            y_pred = output.max(1)[1].detach().cpu().numpy()\n",
        "            correct_batch= np.mean(target.cpu().numpy() == y_pred)\n",
        "            correct  += np.mean(target.cpu().numpy() == y_pred)\n",
        "\n",
        "        ACCURACIES[epoch - 1] = correct/batch_counter\n",
        "        TRAIN_LOSES[epoch - 1] = train_loss\n",
        "        VALID_LOSES[epoch - 1] = valid_loss\n",
        "\n",
        "        # training/validation statistics \n",
        "        print(f'Validation Accuracy: {ACCURACIES[epoch - 1]}, batches count: {batch_counter}' )\n",
        "        print('Epoch: {} \\tTraining Loss: {:.4f} \\tValidation Loss: {:.4f}'.format(\n",
        "            epoch, \n",
        "            train_loss,\n",
        "            valid_loss\n",
        "            ))\n",
        "        \n",
        "        # save the model if validation loss has decreased\n",
        "        if valid_loss < valid_loss_min:\n",
        "            torch.save(model.state_dict(), name_for_save + \"{:.3f}\".format(ACCURACIES[epoch - 1] * 100) + '.pt')\n",
        "            print('Validation loss decreased ({:.4f} --> {:.4f}).  Saving model...'.format(valid_loss_min,valid_loss))\n",
        "            valid_loss_min = valid_loss  \n",
        "        \n",
        "        scheduler.step()\n",
        "\n",
        "    # draw graphics   \n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(ACCURACIES, label = \"Validation accuracy\" )\n",
        "    plt.plot(ACCURACIES_TRAIN, label = \"Train accuracy\" )\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(VALID_LOSES, label = \"Validation loss\" )\n",
        "    plt.plot(TRAIN_LOSES, label = \"Train loss\" )\n",
        "    plt.legend()\n",
        "          \n",
        "    return model\n",
        "\n",
        "# train the model\n",
        "#model_scratch = train(10, dataloaders, model_scratch, optimizer, criterion,  'model_scratch.pt')\n",
        "\n",
        "# load the model that got the best validation accuracy\n",
        "#model_scratch.load_state_dict(torch.load('model_scratch.pt'))  "
      ],
      "metadata": {
        "id": "X4imNWjhfkTz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  Specify model architecture \n",
        "import torchvision\n",
        "model_resnet = models.resnet50(pretrained = True)\n",
        "\n",
        "# Freeze training for all \"features\" layers\n",
        "for param in model_resnet.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# replace the last fully connected layer with a Linnear layer 120 output\n",
        "in_features = model_resnet.fc.in_features\n",
        "model_resnet.fc = nn.Linear(in_features, 120)\n",
        "\n",
        "model_resnet.to(device)\n",
        "\n",
        "from torchsummary import summary\n",
        "summary(model_resnet, input_size=(3, 224, 224))"
      ],
      "metadata": {
        "id": "TxJmk3uJhmpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "model_grad_parameters = filter(lambda p: p.requires_grad, model_resnet.parameters())\n",
        "optimizer = torch.optim.SGD(model_grad_parameters, lr = 0.01, momentum = 0.9)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size = 8, gamma = 0.1)"
      ],
      "metadata": {
        "id": "zgBdyu3KioJq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "model_transfer =  train(n_epochs, dataloaders, model_resnet, optimizer,  criterion, scheduler,'model_res')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXet4W7hjYZJ",
        "outputId": "2a3c879b-3837-4621-c62d-baea566e7a97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 \tBatch: 1 \tTraining Loss: 4.815531\n",
            "Epoch: 1 \tBatch: 101 \tTraining Loss: 4.315544\n",
            "Epoch: 1 \tBatch: 201 \tTraining Loss: 3.685435\n",
            "Epoch: 1 \tBatch: 301 \tTraining Loss: 3.302519\n",
            "Epoch: 1 \tBatch: 401 \tTraining Loss: 3.058180\n",
            "Epoch: 1 \tBatch: 501 \tTraining Loss: 2.891833\n",
            "Train accuracy: 0.3506433823529412, batches count:  544\n",
            "Validation Accuracy: 0.7094851762820512, batches count: 96\n",
            "Epoch: 1 \tTraining Loss: 2.8402 \tValidation Loss: 0.9620\n",
            "Validation loss decreased (inf --> 0.9620).  Saving model...\n",
            "Epoch: 2 \tBatch: 1 \tTraining Loss: 1.798232\n",
            "Epoch: 2 \tBatch: 101 \tTraining Loss: 2.213635\n",
            "Epoch: 2 \tBatch: 201 \tTraining Loss: 2.098965\n",
            "Epoch: 2 \tBatch: 301 \tTraining Loss: 2.074819\n",
            "Epoch: 2 \tBatch: 401 \tTraining Loss: 2.054107\n",
            "Epoch: 2 \tBatch: 501 \tTraining Loss: 2.030387\n",
            "Train accuracy: 0.5053998161764706, batches count:  544\n",
            "Validation Accuracy: 0.7389823717948718, batches count: 96\n",
            "Epoch: 2 \tTraining Loss: 2.0258 \tValidation Loss: 0.9657\n",
            "Epoch: 3 \tBatch: 1 \tTraining Loss: 1.113284\n",
            "Epoch: 3 \tBatch: 101 \tTraining Loss: 2.027784\n",
            "Epoch: 3 \tBatch: 201 \tTraining Loss: 1.928880\n",
            "Epoch: 3 \tBatch: 301 \tTraining Loss: 1.915575\n",
            "Epoch: 3 \tBatch: 401 \tTraining Loss: 1.885153\n",
            "Epoch: 3 \tBatch: 501 \tTraining Loss: 1.895649\n",
            "Train accuracy: 0.5346966911764706, batches count:  544\n",
            "Validation Accuracy: 0.7869591346153846, batches count: 96\n",
            "Epoch: 3 \tTraining Loss: 1.9053 \tValidation Loss: 0.7401\n",
            "Validation loss decreased (0.9620 --> 0.7401).  Saving model...\n",
            "Epoch: 4 \tBatch: 1 \tTraining Loss: 1.322404\n",
            "Epoch: 4 \tBatch: 101 \tTraining Loss: 1.871295\n",
            "Epoch: 4 \tBatch: 201 \tTraining Loss: 1.843862\n",
            "Epoch: 4 \tBatch: 301 \tTraining Loss: 1.845866\n",
            "Epoch: 4 \tBatch: 401 \tTraining Loss: 1.838208\n",
            "Epoch: 4 \tBatch: 501 \tTraining Loss: 1.846251\n",
            "Train accuracy: 0.5502068014705882, batches count:  544\n",
            "Validation Accuracy: 0.7770432692307692, batches count: 96\n",
            "Epoch: 4 \tTraining Loss: 1.8521 \tValidation Loss: 0.7740\n",
            "Epoch: 5 \tBatch: 1 \tTraining Loss: 1.443008\n",
            "Epoch: 5 \tBatch: 101 \tTraining Loss: 1.913339\n",
            "Epoch: 5 \tBatch: 201 \tTraining Loss: 1.856243\n",
            "Epoch: 5 \tBatch: 301 \tTraining Loss: 1.830341\n",
            "Epoch: 5 \tBatch: 401 \tTraining Loss: 1.814959\n",
            "Epoch: 5 \tBatch: 501 \tTraining Loss: 1.801656\n",
            "Train accuracy: 0.5699678308823529, batches count:  544\n",
            "Validation Accuracy: 0.7817508012820512, batches count: 96\n",
            "Epoch: 5 \tTraining Loss: 1.8009 \tValidation Loss: 0.7528\n",
            "Epoch: 6 \tBatch: 1 \tTraining Loss: 1.310663\n",
            "Epoch: 6 \tBatch: 101 \tTraining Loss: 1.925097\n",
            "Epoch: 6 \tBatch: 201 \tTraining Loss: 1.859861\n",
            "Epoch: 6 \tBatch: 301 \tTraining Loss: 1.827529\n",
            "Epoch: 6 \tBatch: 401 \tTraining Loss: 1.791267\n",
            "Epoch: 6 \tBatch: 501 \tTraining Loss: 1.780595\n",
            "Train accuracy: 0.5734145220588235, batches count:  544\n",
            "Validation Accuracy: 0.7946213942307692, batches count: 96\n",
            "Epoch: 6 \tTraining Loss: 1.7841 \tValidation Loss: 0.7206\n",
            "Validation loss decreased (0.7401 --> 0.7206).  Saving model...\n",
            "Epoch: 7 \tBatch: 1 \tTraining Loss: 1.353244\n",
            "Epoch: 7 \tBatch: 101 \tTraining Loss: 1.733609\n",
            "Epoch: 7 \tBatch: 201 \tTraining Loss: 1.724432\n",
            "Epoch: 7 \tBatch: 301 \tTraining Loss: 1.735502\n",
            "Epoch: 7 \tBatch: 401 \tTraining Loss: 1.747555\n",
            "Epoch: 7 \tBatch: 501 \tTraining Loss: 1.744902\n",
            "Train accuracy: 0.5770909926470589, batches count:  544\n",
            "Validation Accuracy: 0.7821013621794872, batches count: 96\n",
            "Epoch: 7 \tTraining Loss: 1.7577 \tValidation Loss: 0.8249\n",
            "Epoch: 8 \tBatch: 1 \tTraining Loss: 1.570065\n",
            "Epoch: 8 \tBatch: 101 \tTraining Loss: 1.725299\n",
            "Epoch: 8 \tBatch: 201 \tTraining Loss: 1.662381\n",
            "Epoch: 8 \tBatch: 301 \tTraining Loss: 1.696356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model preparation for android"
      ],
      "metadata": {
        "id": "FYbq2b-zmxKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "#It's need to copy architecture of model\n",
        "android_model = torchvision.models.resnet50(pretrained = True)\n",
        "\n",
        "# Freeze training for all \"features\" layers\n",
        "for param in android_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# replace the last fully connected layer with a Linnear layer 133 output\n",
        "in_features = android_model.fc.in_features\n",
        "android_model.fc = nn.Linear(in_features, 120)\n",
        "\n",
        "saved_model_file = \"model_transfer.pt\"\n",
        "android_model.load_state_dict(torch.load(saved_model_file))\n",
        "\n",
        "android_model.eval()\n",
        "# optimization for mobile\n",
        "android_model_file = \"model8.pt\"\n",
        "\n",
        "example = torch.rand(1, 3, 224, 224)\n",
        "traced_script_model = torch.jit.trace(android_model, example)\n",
        "optimized_traced_model = optimize_for_mobile(traced_script_model)\n",
        "optimized_traced_model._save_for_lite_interpreter(android_model_file)"
      ],
      "metadata": {
        "id": "aEAGL7dsnMkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Predictions"
      ],
      "metadata": {
        "id": "7df0EsXgoc4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DogBreedTestDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir,  labels ,transform):\n",
        "        'Initialization'\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.label = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.label)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.label is not None:\n",
        "            img_name = '{}.jpg'.format(self.label.iloc[index, 0])\n",
        "            fullname = self.img_dir + img_name\n",
        "            image = Image.open(fullname)\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "num_workers = 12\n",
        "\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "\n",
        "test_img = DogBreedTestDataset('test/', submission, transform = img_transform['test'])\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_img, batch_size, num_workers = num_workers, shuffle = False)\n",
        "\n",
        "\n",
        "def predict(model, loader ):\n",
        "  result = np.zeros((1, 120))\n",
        "\n",
        "  for batch_idx, data in enumerate(loader):\n",
        "    data = data.to(device)\n",
        "    output = model(data)\n",
        "    prob = F.softmax(output, dim = -1).cpu().detach().numpy()\n",
        "    result = np.concatenate((result, prob), axis = 0)\n",
        "    if batch_idx == 0:  \n",
        "      result = np.delete(result, (0), axis=0)\n",
        "    print(f\"{batch_idx} / {len(loader)}\" )\n",
        "\n",
        "  return result \n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "# restore model\n",
        "restored_model = torchvision.models.resnet50(pretrained = True)\n",
        "\n",
        "# Freeze training for all \"features\" layers\n",
        "for param in restored_model.parameters():\n",
        "    param.requires_grad = False\n",
        "    \n",
        "# replace the last fully connected layer with a Linnear layer 120 output\n",
        "in_features = restored_model.fc.in_features\n",
        "restored_model.fc = nn.Linear(in_features, 120)\n",
        "\n",
        "restored_model.load_state_dict(torch.load(\"model_transfer.pt\"))\n",
        "\n",
        "model = restored_model.to(device)\n",
        "\n",
        "pred= predict(model, test_dataloader)\n",
        "pred.shape"
      ],
      "metadata": {
        "id": "xVJT7m8hof_o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}